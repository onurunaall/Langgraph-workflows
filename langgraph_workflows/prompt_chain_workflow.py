from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph_workflows.llm_setup import get_llm_instance

# Define the state schema with descriptive keys
class JokeWorkflowState(TypedDict):
    topic: str # The subject for which the joke will be generated
    joke: str # The initial joke generated by the LLM
    improved_joke: str # The joke after improvement
    final_joke: str # The final polished joke

llm = get_llm_instance()

def generate_initial_joke(state: JokeWorkflowState):
    """First LLM call to generate an initial joke based on the topic."""
    # Generate a joke about the given topic
    response_message = llm.invoke(f"Write a short joke about {state['topic']}")
  
    return {"joke": response_message.content}

def check_for_punchline(state: JokeWorkflowState):
    """Check if the generated joke contains a punchline (i.e., contains punctuation like '?' or '!')."""
    if "?" in state["joke"] or "!" in state["joke"]:
        return "Fail"
    return "Pass"

def improve_generated_joke(state: JokeWorkflowState):
    """Second LLM call to improve the joke by adding wordplay."""
    response_message = llm.invoke(f"Make this joke funnier by adding wordplay: {state['joke']}")
    return {"improved_joke": response_message.content}

def polish_improved_joke(state: JokeWorkflowState):
    """Third LLM call to polish the improved joke with a surprising twist."""
    response_message = llm.invoke(f"Add a surprising twist to this joke: {state['improved_joke']}")
    return {"final_joke": response_message.content}

def run_prompt_chain_workflow():
    """Build, compile, and run the prompt chaining workflow."""
  
    # Create a new state graph using our defined state
    workflow_graph = StateGraph(JokeWorkflowState)
    
    # Add the nodes to the graph
    workflow_graph.add_node("generate_initial_joke", generate_initial_joke)
    workflow_graph.add_node("improve_generated_joke", improve_generated_joke)
    workflow_graph.add_node("polish_improved_joke", polish_improved_joke)
    
    # Connect nodes: start with joke generation, then conditionally improve the joke if needed
    workflow_graph.add_edge(START, "generate_initial_joke")
    workflow_graph.add_conditional_edges("generate_initial_joke",
                                         check_for_punchline,
                                         {"Fail": "improve_generated_joke", "Pass": END}
                                        )
  
    workflow_graph.add_edge("improve_generated_joke", "polish_improved_joke")
    workflow_graph.add_edge("polish_improved_joke", END)
    
    chained_workflow = workflow_graph.compile()
    
    # Invoke the workflow with an initial topic
    workflow_state = chained_workflow.invoke({"topic": "cats"})
    print("Initial joke:")
    print(workflow_state.get("joke", "No joke generated"))
  
    if "improved_joke" in workflow_state:
        print("\nImproved joke:")
        print(workflow_state.get("improved_joke", "No improved joke"))
        print("\nFinal polished joke:")
        print(workflow_state.get("final_joke", "No final joke"))
    else:
        print("Joke passed the quality gate and no improvement was needed.")

if __name__ == "__main__":
    run_prompt_chain_workflow()
